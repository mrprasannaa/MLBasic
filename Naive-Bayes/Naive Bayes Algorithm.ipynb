{"cells":[{"cell_type":"markdown","id":"da0d92e8","metadata":{"id":"da0d92e8"},"source":["# Naive Bayes"]},{"cell_type":"markdown","id":"826b893d","metadata":{"id":"826b893d"},"source":["##### Bayes’s Theorem\n","\n","According to the Wikipedia, In probability theory and statistics,** Bayes’s theorem** (alternatively *Bayes’s law* or *Bayes’s rule*) describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n","Mathematically, it can be written as:\n","\n","![formula.jpeg](attachment:formula.jpeg)\n","\n","Where A and B are events and P(B)≠0\n","* P(A|B) is a conditional probability: the likelihood of event A occurring given that B is true.\n","* P(B|A) is also a conditional probability: the likelihood of event B occurring given that A is true.\n","* P(A) and P(B) are the probabilities of observing A and B respectively; they are known as the marginal probability.\n"]},{"cell_type":"markdown","id":"90308691","metadata":{"id":"90308691"},"source":["Let’s understand it with the help of an example:\n","\n","**The problem statement:**\n","\n","You are planning a picnic today, but the morning is cloudy\n","\n","Oh no! 50% of all rainy days start off cloudy!\n","But cloudy mornings are common (about 40% of days start cloudy)\n","And this is usually a dry month (only 3 of 30 days tend to be rainy, or 10%)\n","What is the chance of rain during the day?\n","\n","We will use Rain to mean rain during the day, and Cloud to mean cloudy morning.\n","\n","The chance of Rain given Cloud is written P(Rain|Cloud)\n","\n","So let's put that in the formula:\n","\n","$P(Rain|Cloud) = \\frac{P(Rain)*P(Cloud|Rain)} {P(Cloud)}$          \n","                      \n"," \n","\n","- P(Rain) is Probability of Rain = 10%\n","- P(Cloud|Rain) is Probability of Cloud, given that Rain happens = 50%\n","- P(Cloud) is Probability of Cloud = 40%\n","\n","$P(Rain|Cloud) =  \\frac{(0.1 x 0.5)} {0.4}   = .125$\n","\n","Or a 12.5% chance of rain. Not too bad, let's have a picnic!"]},{"cell_type":"markdown","id":"8335e701","metadata":{"id":"8335e701"},"source":["**Naïve:** It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.<br>\n","**Bayes:** It is called Bayes because it depends on the principle of Bayes' Theorem"]},{"cell_type":"markdown","id":"fd700508","metadata":{"id":"fd700508"},"source":["# Problem statement"]},{"cell_type":"markdown","id":"0b0ed084","metadata":{"id":"0b0ed084"},"source":["Spam filtering using naive Bayes classifiers in order to predict whether a new mail based on its content, can be categorized as spam or not-spam."]},{"cell_type":"markdown","id":"cf0a2d63","metadata":{"id":"cf0a2d63"},"source":["### Data processing using panda library"]},{"cell_type":"code","execution_count":null,"id":"b7fb5951","metadata":{"id":"b7fb5951"},"outputs":[],"source":["# Import the required libraries\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","import string\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"cf8291fe","metadata":{"id":"cf8291fe","outputId":"0911e369-5bc8-4119-fae6-7e9367c81f58"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>Message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>I've been searching for the right words to tha...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>Even my brother is not like to speak with me. ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>I HAVE A DATE ON SUNDAY WITH WILL!!!</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ham</td>\n","      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>spam</td>\n","      <td>WINNER!! As a valued network customer you have...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>spam</td>\n","      <td>Had your mobile 11 months or more? U R entitle...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Class                                            Message\n","0   ham  I've been searching for the right words to tha...\n","1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n","2   ham  Nah I don't think he goes to usf, he lives aro...\n","3   ham  Even my brother is not like to speak with me. ...\n","4   ham               I HAVE A DATE ON SUNDAY WITH WILL!!!\n","5   ham  As per your request 'Melle Melle (Oru Minnamin...\n","6  spam  WINNER!! As a valued network customer you have...\n","7  spam  Had your mobile 11 months or more? U R entitle..."]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Load the dataset\n","\n","data = pd.read_csv(\"spam.tsv\",sep='\\t',names=['Class','Message'])\n","data.head(8) # View the first 8 records of our dataset"]},{"cell_type":"code","execution_count":null,"id":"0ca2dbb5","metadata":{"id":"0ca2dbb5","outputId":"4c372cfd-8d22-4f5b-9d55-9d38764369de"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>Message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>I've been searching for the right words to tha...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Class                                            Message\n","0   ham  I've been searching for the right words to tha..."]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# to view the first record\n","data.loc[:0]"]},{"cell_type":"code","execution_count":null,"id":"19568f8e","metadata":{"id":"19568f8e","outputId":"d658fc40-3153-40ac-916c-91f018321836"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5567 entries, 0 to 5566\n","Data columns (total 2 columns):\n"," #   Column   Non-Null Count  Dtype \n","---  ------   --------------  ----- \n"," 0   Class    5567 non-null   object\n"," 1   Message  5567 non-null   object\n","dtypes: object(2)\n","memory usage: 87.1+ KB\n"]}],"source":["# Summary of the dataset\n","data.info()"]},{"cell_type":"code","execution_count":null,"id":"4748940c","metadata":{"id":"4748940c"},"outputs":[],"source":["# create a column to keep the count of the characters present in each record\n","data['Length'] = data['Message'].apply(len)"]},{"cell_type":"code","execution_count":null,"id":"9d919c62","metadata":{"id":"9d919c62","outputId":"5e0756d2-c187-48d9-ff72-44be9e7355ee"},"outputs":[{"data":{"text/plain":["0       196\n","1       155\n","2        61\n","3        77\n","4        36\n","       ... \n","5562    160\n","5563     36\n","5564     57\n","5565    125\n","5566     26\n","Name: Length, Length: 5567, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data['Length']"]},{"cell_type":"code","execution_count":null,"id":"fe8316bf","metadata":{"id":"fe8316bf","outputId":"f3c55b03-43eb-4ed9-f4f5-0608797420df"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>Message</th>\n","      <th>Length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>I've been searching for the right words to tha...</td>\n","      <td>196</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>155</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>61</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>Even my brother is not like to speak with me. ...</td>\n","      <td>77</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>I HAVE A DATE ON SUNDAY WITH WILL!!!</td>\n","      <td>36</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ham</td>\n","      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n","      <td>160</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>spam</td>\n","      <td>WINNER!! As a valued network customer you have...</td>\n","      <td>157</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>spam</td>\n","      <td>Had your mobile 11 months or more? U R entitle...</td>\n","      <td>154</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>ham</td>\n","      <td>I'm gonna be home soon and i don't want to tal...</td>\n","      <td>109</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>spam</td>\n","      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n","      <td>136</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Class                                            Message  Length\n","0   ham  I've been searching for the right words to tha...     196\n","1  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n","2   ham  Nah I don't think he goes to usf, he lives aro...      61\n","3   ham  Even my brother is not like to speak with me. ...      77\n","4   ham               I HAVE A DATE ON SUNDAY WITH WILL!!!      36\n","5   ham  As per your request 'Melle Melle (Oru Minnamin...     160\n","6  spam  WINNER!! As a valued network customer you have...     157\n","7  spam  Had your mobile 11 months or more? U R entitle...     154\n","8   ham  I'm gonna be home soon and i don't want to tal...     109\n","9  spam  SIX chances to win CASH! From 100 to 20,000 po...     136"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# view the dataset with the column 'Length' which contains the number of characters present in each mail\n","data.head(10)"]},{"cell_type":"code","execution_count":null,"id":"11067196","metadata":{"id":"11067196","outputId":"803060ee-1375-4588-dfaf-302c6b97cacc"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Message</th>\n","      <th>Length</th>\n","    </tr>\n","    <tr>\n","      <th>Class</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ham</th>\n","      <td>4821</td>\n","      <td>4821</td>\n","    </tr>\n","    <tr>\n","      <th>spam</th>\n","      <td>746</td>\n","      <td>746</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Message  Length\n","Class                 \n","ham       4821    4821\n","spam       746     746"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["## The mails are categorised into 2 classes ie., spam and ham. \n","# Let's see the count of each class\n","data.groupby('Class').count()"]},{"cell_type":"markdown","id":"b2aee521","metadata":{"id":"b2aee521"},"source":["### Data Visualization"]},{"cell_type":"code","execution_count":null,"id":"15106dfb","metadata":{"id":"15106dfb","outputId":"042a658a-590b-4f4b-8e8e-8f38e297c556"},"outputs":[{"data":{"text/plain":["count    5567.000000\n","mean       80.450153\n","std        59.891023\n","min         2.000000\n","25%        36.000000\n","50%        62.000000\n","75%       122.000000\n","max       910.000000\n","Name: Length, dtype: float64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["data['Length'].describe() # to find the max length of the message. "]},{"cell_type":"markdown","id":"f488e9f0","metadata":{"id":"f488e9f0"},"source":["See what we found, A 910 character long message. Let's use masking to find this message:"]},{"cell_type":"code","execution_count":null,"id":"13a14478","metadata":{"id":"13a14478","outputId":"f97d4409-bc3e-49e9-9737-9b2edb86b75e"},"outputs":[{"data":{"text/plain":["0       False\n","1       False\n","2       False\n","3       False\n","4       False\n","        ...  \n","5562    False\n","5563    False\n","5564    False\n","5565    False\n","5566    False\n","Name: Length, Length: 5567, dtype: bool"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["data['Length']==910"]},{"cell_type":"code","execution_count":null,"id":"8b1c68dc","metadata":{"id":"8b1c68dc","outputId":"28fca1a3-1464-4009-8e90-343271c2140e"},"outputs":[{"data":{"text/plain":["1080    For me the love should start with attraction.i...\n","Name: Message, dtype: object"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# the message that has the max characters\n","data[data['Length']==910]['Message']"]},{"cell_type":"code","execution_count":null,"id":"3ad2381f","metadata":{"id":"3ad2381f","outputId":"cb950c90-d8de-4468-e54e-40481f005e20"},"outputs":[{"data":{"text/plain":["\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# view the message that has 910 characters in it\n","data[data['Length']==910]['Message'].iloc[0]"]},{"cell_type":"code","execution_count":null,"id":"0dbd6254","metadata":{"id":"0dbd6254","outputId":"ebc9f030-f008-4f79-8d28-85acc51c0f54"},"outputs":[{"data":{"text/plain":["'Ok'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# View the message that has min characters\n","data[data['Length']==2]['Message'].iloc[0]"]},{"cell_type":"code","source":[""],"metadata":{"id":"moa8k8RdhEWC"},"id":"moa8k8RdhEWC","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"fe983a86","metadata":{"id":"fe983a86"},"source":["### Text Pre-Processing"]},{"cell_type":"code","execution_count":null,"id":"699f0dd9","metadata":{"id":"699f0dd9","outputId":"f1c160eb-91fb-4e19-a78a-d08388b65de4"},"outputs":[{"data":{"text/plain":["array(['ham', 'spam', 'ham', ..., 'ham', 'ham', 'ham'], dtype=object)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# creating an object for the target values\n","dObject = data['Class'].values\n","dObject"]},{"cell_type":"code","execution_count":null,"id":"37938cfc","metadata":{"id":"37938cfc"},"outputs":[],"source":["# Lets assign ham as 1\n","data.loc[data['Class']==\"ham\",\"Class\"] = 1"]},{"cell_type":"code","execution_count":null,"id":"82e5e698","metadata":{"id":"82e5e698"},"outputs":[],"source":["# Lets assign spam as 0\n","data.loc[data['Class']==\"spam\",\"Class\"] = 0"]},{"cell_type":"code","execution_count":null,"id":"7a2cdf80","metadata":{"id":"7a2cdf80","outputId":"7b0e4951-c0e3-430b-9fbf-59ec0e3a27d1"},"outputs":[{"data":{"text/plain":["array([1, 0, 1, ..., 1, 1, 1], dtype=object)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["dObject2=data['Class'].values\n","dObject2"]},{"cell_type":"code","execution_count":null,"id":"92685aba","metadata":{"id":"92685aba","outputId":"e82b6ba7-ddfc-48c2-f7f1-428302eec6c0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>Message</th>\n","      <th>Length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>I've been searching for the right words to tha...</td>\n","      <td>196</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>155</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>61</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>Even my brother is not like to speak with me. ...</td>\n","      <td>77</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>I HAVE A DATE ON SUNDAY WITH WILL!!!</td>\n","      <td>36</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n","      <td>160</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>WINNER!! As a valued network customer you have...</td>\n","      <td>157</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>Had your mobile 11 months or more? U R entitle...</td>\n","      <td>154</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Class                                            Message  Length\n","0     1  I've been searching for the right words to tha...     196\n","1     0  Free entry in 2 a wkly comp to win FA Cup fina...     155\n","2     1  Nah I don't think he goes to usf, he lives aro...      61\n","3     1  Even my brother is not like to speak with me. ...      77\n","4     1               I HAVE A DATE ON SUNDAY WITH WILL!!!      36\n","5     1  As per your request 'Melle Melle (Oru Minnamin...     160\n","6     0  WINNER!! As a valued network customer you have...     157\n","7     0  Had your mobile 11 months or more? U R entitle...     154"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["data.head(8)"]},{"cell_type":"markdown","id":"05eb4032","metadata":{"id":"05eb4032"},"source":["First removing punctuation. We can just take advantage of Python's built-in string library to get a quick list of all the possible punctuation:"]},{"cell_type":"code","execution_count":null,"id":"6a3314d3","metadata":{"id":"6a3314d3","outputId":"9c0a95eb-3efe-46a3-e44c-2a7196c5bf65"},"outputs":[{"data":{"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# the default list of punctuations\n","import string\n","\n","string.punctuation"]},{"cell_type":"code","execution_count":null,"id":"f0b90398","metadata":{"id":"f0b90398","outputId":"ba3ebcc3-ad21-41b0-cef6-5e4ad492df7e"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Why is it important to remove punctuation?\n","\n","\"This message is spam\" == \"This message is spam.\""]},{"cell_type":"code","execution_count":null,"id":"648fdbcd","metadata":{"id":"648fdbcd","outputId":"2d42e1e7-8ed6-4dd2-87b9-617e6e151435"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>Message</th>\n","      <th>Length</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>I've been searching for the right words to tha...</td>\n","      <td>196</td>\n","      <td>Ive been searching for the right words to than...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>155</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>61</td>\n","      <td>Nah I dont think he goes to usf he lives aroun...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>Even my brother is not like to speak with me. ...</td>\n","      <td>77</td>\n","      <td>Even my brother is not like to speak with me T...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>I HAVE A DATE ON SUNDAY WITH WILL!!!</td>\n","      <td>36</td>\n","      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Class                                            Message  Length  \\\n","0     1  I've been searching for the right words to tha...     196   \n","1     0  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n","2     1  Nah I don't think he goes to usf, he lives aro...      61   \n","3     1  Even my brother is not like to speak with me. ...      77   \n","4     1               I HAVE A DATE ON SUNDAY WITH WILL!!!      36   \n","\n","                                          text_clean  \n","0  Ive been searching for the right words to than...  \n","1  Free entry in 2 a wkly comp to win FA Cup fina...  \n","2  Nah I dont think he goes to usf he lives aroun...  \n","3  Even my brother is not like to speak with me T...  \n","4                  I HAVE A DATE ON SUNDAY WITH WILL  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Let's remove the punctuation\n","\n","def remove_punct(text):\n","    text = \"\".join([char for char in text if char not in string.punctuation])\n","    return text\n","\n","data['text_clean'] = data['Message'].apply(lambda x: remove_punct(x))\n","\n","data.head()"]},{"cell_type":"markdown","id":"6f1587b6","metadata":{"id":"6f1587b6"},"source":["__Tokenization__ (process of converting the normal text strings in to a list of tokens(also known as lemmas))."]},{"cell_type":"code","execution_count":null,"id":"277356ea","metadata":{"id":"277356ea","outputId":"f43f07dd-eda3-4a0b-9ad8-8b96dbcb4cbe"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>Message</th>\n","      <th>Length</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>I've been searching for the right words to tha...</td>\n","      <td>196</td>\n","      <td>Ive been searching for the right words to than...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>155</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>61</td>\n","      <td>Nah I dont think he goes to usf he lives aroun...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>Even my brother is not like to speak with me. ...</td>\n","      <td>77</td>\n","      <td>Even my brother is not like to speak with me T...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>I HAVE A DATE ON SUNDAY WITH WILL!!!</td>\n","      <td>36</td>\n","      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n","      <td>160</td>\n","      <td>As per your request Melle Melle Oru Minnaminun...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>WINNER!! As a valued network customer you have...</td>\n","      <td>157</td>\n","      <td>WINNER As a valued network customer you have b...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>Had your mobile 11 months or more? U R entitle...</td>\n","      <td>154</td>\n","      <td>Had your mobile 11 months or more U R entitled...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Class                                            Message  Length  \\\n","0     1  I've been searching for the right words to tha...     196   \n","1     0  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n","2     1  Nah I don't think he goes to usf, he lives aro...      61   \n","3     1  Even my brother is not like to speak with me. ...      77   \n","4     1               I HAVE A DATE ON SUNDAY WITH WILL!!!      36   \n","5     1  As per your request 'Melle Melle (Oru Minnamin...     160   \n","6     0  WINNER!! As a valued network customer you have...     157   \n","7     0  Had your mobile 11 months or more? U R entitle...     154   \n","\n","                                          text_clean  \n","0  Ive been searching for the right words to than...  \n","1  Free entry in 2 a wkly comp to win FA Cup fina...  \n","2  Nah I dont think he goes to usf he lives aroun...  \n","3  Even my brother is not like to speak with me T...  \n","4                  I HAVE A DATE ON SUNDAY WITH WILL  \n","5  As per your request Melle Melle Oru Minnaminun...  \n","6  WINNER As a valued network customer you have b...  \n","7  Had your mobile 11 months or more U R entitled...  "]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# original text and cleaned text\n","data.head(8)"]},{"cell_type":"markdown","id":"b3e4ae51","metadata":{"id":"b3e4ae51"},"source":["Now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with and machine learning model which we will gonig to use can understand."]},{"cell_type":"code","execution_count":null,"id":"c21b667f","metadata":{"id":"c21b667f"},"outputs":[],"source":["# Countvectorizer is a method to convert text to numerical data. \n","\n","# Initialize the object for countvectorizer \n","CV = CountVectorizer(stop_words=\"english\")"]},{"cell_type":"markdown","id":"2bd1abc9","metadata":{"id":"2bd1abc9"},"source":["[Stopwords are the words in any language which does not add much meaning to a sentence. They are the words which are very common in text documents such as a, an, the, you, your, etc. The Stop Words highly appear in text documents. However, they are not being helpful for text analysis in many of the cases, So it is better to remove from the text. We can focus on the important words if stop words have removed.]"]},{"cell_type":"code","execution_count":null,"id":"de16e620","metadata":{"id":"de16e620","outputId":"9a3900c7-dec4-4a66-80d2-34e7fdfbf1fa"},"outputs":[{"data":{"text/plain":["array([1, 0, 1, ..., 1, 1, 1], dtype=object)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# Splitting x and y\n","\n","xSet = data['text_clean'].values\n","ySet = data['Class'].values\n","ySet"]},{"cell_type":"code","execution_count":null,"id":"62187f2d","metadata":{"id":"62187f2d","outputId":"56b22477-2e95-4a71-855f-a93e9785738f"},"outputs":[{"data":{"text/plain":["array([1, 0, 1, ..., 1, 1, 1])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Datatype for y is object. lets convert it into int\n","ySet = ySet.astype('int')\n","ySet"]},{"cell_type":"code","execution_count":null,"id":"0320d8d8","metadata":{"id":"0320d8d8","outputId":"00ee53e5-2a9e-43f7-88d7-7b55ee6f445d"},"outputs":[{"data":{"text/plain":["array(['Ive been searching for the right words to thank you for this breather I promise i wont take your help for granted and will fulfil my promise You have been wonderful and a blessing at all times',\n","       'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s',\n","       'Nah I dont think he goes to usf he lives around here though', ...,\n","       'Pity  was in mood for that Soany other suggestions',\n","       'The guy did some bitching but I acted like id be interested in buying something else next week and he gave it to us for free',\n","       'Rofl Its true to its name'], dtype=object)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["xSet"]},{"cell_type":"markdown","id":"0a234c70","metadata":{"id":"0a234c70"},"source":["### Splitting Train and Test Data"]},{"cell_type":"code","execution_count":null,"id":"c8042a44","metadata":{"id":"c8042a44"},"outputs":[],"source":["xSet_train,xSet_test,ySet_train,ySet_test = train_test_split(xSet,ySet,test_size=0.2, random_state=10)"]},{"cell_type":"code","execution_count":null,"id":"367d60a0","metadata":{"id":"367d60a0","outputId":"5d11dfd4-aa9b-4f0e-90df-3afc9fa0831a"},"outputs":[{"data":{"text/plain":["<4453x8159 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 34532 stored elements in Compressed Sparse Row format>"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["\n","xSet_train_CV = CV.fit_transform(xSet_train)\n","xSet_train_CV"]},{"cell_type":"markdown","id":"95af9203","metadata":{"id":"95af9203"},"source":["### Training a model\n","\n","With messages represented as vectors, we can finally train our spam/ham classifier. Now we can actually use almost any sort of classification algorithms. For a variety of reasons, the Naive Bayes classifier algorithm is a good choice."]},{"cell_type":"code","execution_count":null,"id":"e3db2196","metadata":{"id":"e3db2196"},"outputs":[],"source":["# Initialising the model\n","NB = MultinomialNB()"]},{"cell_type":"code","execution_count":null,"id":"488a58ea","metadata":{"id":"488a58ea","outputId":"e2cdcf8b-448c-489a-e4e2-263b27a710fe"},"outputs":[{"data":{"text/plain":["MultinomialNB()"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# feed data to the model\n","NB.fit(xSet_train_CV,ySet_train)"]},{"cell_type":"code","execution_count":null,"id":"9253aead","metadata":{"id":"9253aead"},"outputs":[],"source":["# Let's test CV on our test data\n","xSet_test_CV = CV.transform(xSet_test)"]},{"cell_type":"code","execution_count":null,"id":"8576ddb9","metadata":{"id":"8576ddb9","outputId":"44821dbc-3988-4a5b-e0a9-e3a09fd50128"},"outputs":[{"data":{"text/plain":["array([1, 1, 1, ..., 1, 1, 1])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# prediction for xSet_test_CV\n","\n","ySet_predict = NB.predict(xSet_test_CV)\n","ySet_predict"]},{"cell_type":"code","execution_count":null,"id":"a4b67c9f","metadata":{"id":"a4b67c9f","outputId":"09ae33dd-0dd0-4e69-cff0-2ba020f5078d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction Accuracy : 98.29443447037703\n"]}],"source":["# Checking accuracy\n","\n","accuracyScore = accuracy_score(ySet_test,ySet_predict)*100\n","\n","print(\"Prediction Accuracy :\",accuracyScore)"]},{"cell_type":"markdown","id":"71729726","metadata":{"id":"71729726"},"source":["### SpamClassificationApplication"]},{"cell_type":"code","execution_count":null,"id":"2c304166","metadata":{"id":"2c304166","outputId":"4ac1976e-909b-4faa-a073-bd6230b36a93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Message: \n","---------------------------MESSAGE-SENT-[CHECK-INBOX]------------------------------\n"]}],"source":["msg = input(\"Enter Message: \") # to get the input message\n","msgInput = CV.transform([msg]) # \n","predict = NB.predict(msgInput)\n","if(predict[0]==0):\n","    print(\"------------------------MESSAGE-SENT-[CHECK-SPAM-FOLDER]---------------------------\")\n","else:\n","    print(\"---------------------------MESSAGE-SENT-[CHECK-INBOX]------------------------------\")"]},{"cell_type":"markdown","id":"6c21a363","metadata":{"id":"6c21a363"},"source":["## BAG OF WORDS"]},{"cell_type":"markdown","id":"3153a594","metadata":{"id":"3153a594"},"source":["We cannot pass text directly to train our models in Natural Language Processing, thus we need to convert it into numbers, which machine can understand and can perform the required modelling on it"]},{"cell_type":"code","execution_count":null,"id":"ad5ad021","metadata":{"id":"ad5ad021"},"outputs":[],"source":["# Let's understand it with a simple example"]},{"cell_type":"code","execution_count":null,"id":"924c35d9","metadata":{"id":"924c35d9","outputId":"30186308-e675-4bc7-80be-0158c377012f"},"outputs":[{"data":{"text/plain":["'man eats food'"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["# creating a list of sentences\n","documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n","\n","# Changing the text to lower case and remove the full stop from text\n","processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n","processed_docs[3]"]},{"cell_type":"code","execution_count":null,"id":"3a895c81","metadata":{"id":"3a895c81","outputId":"ec0340b8-e524-4902-895c-cb6afafd2784"},"outputs":[{"name":"stdout","output_type":"stream","text":["Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n","Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n","BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n","BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n","Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"]}],"source":["# corpus is the collection of text\n","#look at the documents list\n","print(\"Our corpus: \", processed_docs)\n","\n","\n","# Initialise the object for CountVectorizer\n","count_vect = CountVectorizer()\n","\n","#Build a BOW representation for the corpus\n","bow_rep = count_vect.fit_transform(processed_docs)\n","\n","#Look at the vocabulary mapping\n","print(\"Our vocabulary: \", count_vect.vocabulary_)\n","\n","#see the BOW rep for first 2 documents\n","print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n","print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n","\n","#Get the representation using this vocabulary, for a new text\n","temp = count_vect.transform([\"dog and dog are friends\"])\n","print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"]},{"cell_type":"markdown","id":"548989e4","metadata":{"id":"548989e4"},"source":["## TF-IDF"]},{"cell_type":"markdown","id":"b748d08a","metadata":{"id":"b748d08a"},"source":["In **BOW approach** we saw so far, all the words in the text are treated equally important. There is no notion of some words in the document being more important than others. TF-IDF addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the \n","\n","\n","<font color=darkviolet>  **Term Frequency (tf)** </font>\n","TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n","\n","TF(t) = (Number of times term 't' appears in a document) / (Total number of terms in the document).\n","\n","\n","\n","<font color=darkviolet>  **Inverse Document Frequency (idf)** </font>\n","              It measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n","\n","IDF(t) = log_e(Total number of documents / Number of documents with term t in it).corpus. It was commonly used representation scheme for information retrieval systems, for extracting relevant documents from a corpus for given text query.\n","\n","\n","\n","__Let's see an example:__\n","\n","Consider a document containing 100 words wherein the word cat appears 3 times. \n","\n","The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. \n","\n","Now, assume we have 10 million documents and the word cat appears in one thousand of these. \n","\n","Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. \n","\n","Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12"]},{"cell_type":"code","execution_count":null,"id":"5ac8e838","metadata":{"id":"5ac8e838","outputId":"da9e59e5-7570-4a17-cfd3-81be528ce51a"},"outputs":[{"data":{"text/plain":["array([1, 0, 1, ..., 1, 1, 1], dtype=object)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# Splitting x and y\n","\n","X = data['text_clean'].values\n","y = data['Class'].values\n","y"]},{"cell_type":"code","execution_count":null,"id":"c33fcff8","metadata":{"id":"c33fcff8","outputId":"0fe5e0d7-a420-4280-c0e3-03cf0eb23e72"},"outputs":[{"data":{"text/plain":["array([1, 0, 1, ..., 1, 1, 1])"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# Datatype for y is object. lets convert it into int\n","y = y.astype('int')\n","y"]},{"cell_type":"code","execution_count":null,"id":"13221397","metadata":{"id":"13221397","outputId":"61a456a3-8628-4821-f848-1f073f3f3ba9"},"outputs":[{"data":{"text/plain":["numpy.ndarray"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["type(X)"]},{"cell_type":"code","execution_count":null,"id":"762dfa74","metadata":{"id":"762dfa74"},"outputs":[],"source":["## text preprocessing and feature vectorizer\n","# To extract features from a document of words, we import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","tf=TfidfVectorizer() ## object creation\n","X=tf.fit_transform(X) ## fitting and transforming the data into vectors\n"]},{"cell_type":"code","execution_count":null,"id":"9c2808f0","metadata":{"id":"9c2808f0","outputId":"47a70e1c-f433-44a9-f039-ad96bf6dc210"},"outputs":[{"data":{"text/plain":["(5567, 9537)"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"id":"309c0346","metadata":{"id":"309c0346"},"outputs":[],"source":["## print feature names selected from the raw documents\n","tf.get_feature_names()"]},{"cell_type":"code","execution_count":null,"id":"422b4beb","metadata":{"id":"422b4beb","outputId":"c170d03a-2493-4afb-b823-f912f195b2db"},"outputs":[{"data":{"text/plain":["9537"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["## number of features created\n","len(tf.get_feature_names())"]},{"cell_type":"code","execution_count":null,"id":"e0cd41c9","metadata":{"id":"e0cd41c9","outputId":"5740b90a-f669-4f89-bceb-58ea128642e5"},"outputs":[{"data":{"text/plain":["<5567x9537 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 72701 stored elements in Compressed Sparse Row format>"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["X"]},{"cell_type":"code","execution_count":null,"id":"80d85c70","metadata":{"id":"80d85c70"},"outputs":[],"source":["## getting the feature vectors\n","X=X.toarray()"]},{"cell_type":"code","execution_count":null,"id":"cee83ab4","metadata":{"id":"cee83ab4"},"outputs":[],"source":["## Creating training and testing\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=6)"]},{"cell_type":"code","execution_count":null,"id":"069e2d1b","metadata":{"id":"069e2d1b"},"outputs":[],"source":["## Model creation\n","from sklearn.naive_bayes import BernoulliNB\n","\n","## model object creation\n","nb=BernoulliNB(alpha=0.01) \n","\n","## fitting the model\n","nb.fit(X_train,y_train)\n","\n","## getting the prediction\n","y_hat=nb.predict(X_test) "]},{"cell_type":"code","execution_count":null,"id":"ba47b196","metadata":{"id":"ba47b196","outputId":"83c19007-4fc3-48bc-9635-82f6640674b0"},"outputs":[{"data":{"text/plain":["array([1, 1, 1, ..., 1, 1, 1])"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["y_hat"]},{"cell_type":"code","execution_count":null,"id":"e0a96df8","metadata":{"id":"e0a96df8"},"outputs":[],"source":["## Evaluating the model\n","from sklearn.metrics import classification_report,confusion_matrix"]},{"cell_type":"code","execution_count":null,"id":"78a41f63","metadata":{"id":"78a41f63","outputId":"20f16024-4e9e-4941-ea38-cd2b4c71d95e"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.96      0.96       186\n","           1       0.99      0.99      0.99      1206\n","\n","    accuracy                           0.99      1392\n","   macro avg       0.98      0.98      0.98      1392\n","weighted avg       0.99      0.99      0.99      1392\n","\n"]}],"source":["print(classification_report(y_test,y_hat))"]},{"cell_type":"code","execution_count":null,"id":"78973b0c","metadata":{"id":"78973b0c","outputId":"099cc473-2394-4228-8969-28f18cd50d66"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>col_0</th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","    <tr>\n","      <th>row_0</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>178</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>1199</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["col_0    0     1\n","row_0           \n","0      178     8\n","1        7  1199"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["## confusion matrix\n","pd.crosstab(y_test,y_hat)"]},{"cell_type":"markdown","id":"711d46ab","metadata":{"id":"711d46ab"},"source":["### Pros of Naive Bayes\n","\n","- Naive Bayes Algorithm is a fast, highly scalable algorithm\n","- Naive Bayes can be classified for both binary classification and multi class classification. It provides different types of Naive Bayes Algorithms like GaussianNB, MultinominalNB, BernoulliNB.\n","- It is simple algorithm that depends on doing a bunch of count.\n","- Great choice for text classification problems. it's a popular choice for spam email classification.\n","- It can be easily trained on small datasets.\n","- Naive Bayes can handle misssing data, as they ignored when a probabilty is calculated for a class value.\n"]},{"cell_type":"markdown","id":"b94fe79a","metadata":{"id":"b94fe79a"},"source":["### Cons of Naive Bayes\n","\n","- It considers all the features to be unrelated, so it cannot learn the relationship between features. This limits the applicability of this algorithm in real-world use cases.\n","- Naive Bayes can learn individual featutre importance but can't determine the relationship among features. "]},{"cell_type":"markdown","id":"73a59c27","metadata":{"id":"73a59c27"},"source":["## Application of Naive Bayes\n","\n","##### Text classification / spam filtering / Sentiment analysis:\n"," - Naive Bayes classifiers mostly used in text classification\n"," - News article classification SPORTS, TECHNOLOGY etc.\n"," - Spam or Ham: Naive Bayes is the most popular method for mail filtering\n"," - Sentiment analysis focuses on identifying whether the customers think positively or negatively about a certain topic (product or service).\n"," \n"," \n","##### Recommendation System:\n","- Naive Bayes classifier and Collabrative filtering together buids a recommendation system that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not. \n","\n"]},{"cell_type":"markdown","id":"8d3d1e11","metadata":{"id":"8d3d1e11"},"source":["### 3 Types of Naive Bayes in Scikit Learn\n","\n","__Gaussian__\n","\n","- It is used in classification and it assumes that features follow a normal distribution.\n","\n","__Multinominal__\n","- It is used for discrete counts. For eg., let's say we have a text cLassification problem. Here we consider Bernoulli trails which is one step further and instead of \"word occuring in the document\", we have \"count how often word occurs in the document\" you can think of it as \"number of times outcome number_x is observed over n trails\".\n","\n","__Bernoulli__\n","- The binomial model is useful if your feature vectors are binary (ie., Zeroes and One). One application would be text classification with 'bag of words' model where the 1s and 0s are \"words occur in the document\" and \"word does not occur in the document\" respectively."]},{"cell_type":"code","execution_count":null,"id":"60f14ecb","metadata":{"id":"60f14ecb"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Naive Bayes Algorithm.ipynb","provenance":[],"collapsed_sections":["b2aee521","fe983a86","0a234c70","95af9203","71729726","711d46ab","b94fe79a","8d3d1e11"]}},"nbformat":4,"nbformat_minor":5}